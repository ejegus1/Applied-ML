{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: K-Nearest-Neighbors\n",
    "\n",
    "Two plants that look very much alike probably represent the same species; likewise, it is quite common that patients complaining of similar symptoms suffer from the same disease. In short, similar objects often belong to the same class — an observation that forms the basis of a popular approach to classification: when asked to determine the class of object $x$ find the training example most similar to it. Then label $x$ with this example’s class.  This is the fundamental notion of ___K-Nearest-Neighbors___.\n",
    "\n",
    "# K-Nearest-Neighbors Algorithm\n",
    "\n",
    "Let's discuss the simplest version of the K-NN classifier algorithm.  Suppose we have a mechanism to evaluate the similarly between attribute vectors - such as Eucliedean distance. Let $x$ denote the object whose class we want to determine.\n",
    "\n",
    "1. Among the training examples, identify the $k$ nearest neighbors of $x$ (examples most similar to $x$). \n",
    "2. Let $c_i$ be the class most frequently found among these $k$ nearest neighbors.  \n",
    "3. Label $x$ with $c_i$.\n",
    "\n",
    "<img src=\"images/03_23.png\" alt=\"knn symbolic\" title=\"title\" width=400/>\n",
    "\n",
    "# The Need for Multiple Neighbors\n",
    "\n",
    "In noisy domains, the testimony of the nearest neighbor cannot be trusted. What if this single specimen’s class label is incorrect? A more robust approach identifies not one, but several nearest neighbors, and then lets them vote. This is the essence of the so-called k-NN classifier, where k is the number of the voting neighbors, which is a user-specified parameter.\n",
    "\n",
    "<img src=\"Figures/nearestNeighbors.png\" alt=\"knn symbolic\" title=\"title\" width=600/>\n",
    "\n",
    "# Measuring Similarity\n",
    "\n",
    "The most common similarity mesure is the Euclidean distance, defined as:\n",
    "\n",
    "$$ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_n - y_n)^2} $$\n",
    "\n",
    "which is equivalent to:\n",
    "\n",
    "$$ d(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "\n",
    "# Irrelevant Attributes and Scaling Problems\n",
    "\n",
    "Some features are irrelevant in the sense that their values have nothing to do with the class to which the instance they describe belongs.\n",
    "\n",
    "Let’s consider an example. In the following figure, the training set examples are characterized by two numeric attributes: body-temperature (the horizontal axis) and shoe-size (the vertical axis). The black dot stands for the object that the K-NN classifier is expected to label as healthy (pos) or sick (neg) - that is, the black dot is the new instance that we want to make a prediction on.\n",
    "\n",
    "<img src=\"Figures/shoeSizeBodyTemp.png\" title=\"title\" width=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Iris Data Set\n",
    "\n",
    "## Load Data\n",
    "\n",
    "Let's import the Iris dataset and try to predict the type of Iris represented by some particular set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>iris_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     iris_type  \n",
       "0       setosa  \n",
       "1       setosa  \n",
       "2       setosa  \n",
       "3       setosa  \n",
       "4       setosa  \n",
       "..         ...  \n",
       "145  virginica  \n",
       "146  virginica  \n",
       "147  virginica  \n",
       "148  virginica  \n",
       "149  virginica  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# only use sklearn to grab the data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()\n",
    "\n",
    "iris_df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "#adding targets to the df\n",
    "iris_df['iris_type'] = iris_data.target_names[iris_data.target]\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Similarity or Distance for a Test Point and all Training Data\n",
    "\n",
    "`np.linalg.norm(x)` for an one-dimensional array `x` is $\\sqrt{\\sum_{i=1}^n{x_i^2}}$ which is the Euclidean norm of the vector $x$\n",
    "\n",
    "`x_train` is a numpy $n \\times p$ matrix, where there are $n$ data points with $p$ features.  `test_row` is a numpy array of $p$ elements.  `x_train - test_row` automatically subtracts `test_row` from each row of `x_train`.  `np.linalg.norm(x, axis=1)` finds the Euclidean norm of each row and returns an numpy array of size $n$.\n",
    "\n",
    "`np.argsort` does not change the numpy array `distances`, but returns the set of indices so that `distances[sorted_row_indices]` would appear sorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(x_train, test_row, num_neighbors):\n",
    "    distances = np.linalg.norm(x_train - test_row, axis=1)\n",
    "    #get the sorted row indicies\n",
    "    sorted_row_indicies = np.argsort(distances)\n",
    "    return sorted_row_indicies[:num_neighbors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction using 'Majority' Rule\n",
    "\n",
    "In the below, we use a [Python collection](https://docs.python.org/3/library/collections.html#module-collections) called [Counter()](https://docs.python.org/3/library/collections.html#collections.Counter) that takes a list (or actually, any type of iterable data structure) and creates a dictionary-like object having keys and values.  The keys (of the dictionary) correspond to the unique elements of the list passed to it, and the values (of the dictionary) are computed as the number of times the element appears.\n",
    "\n",
    "`y_train[neighbor_indices]` is the array of target values corresponding to the nearest neighbors.\n",
    "\n",
    "The method `most_common(k)` selects the k elements that have the highest counts, in descending order of counts.  `most_common(k)` returns a list of tuples of the form (element, count).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Make a prediction with neighbors\n",
    "def predict_classification(x_train, y_train, test_row, num_neighbors):\n",
    "    neighbor_indicies = get_neighbors(x_train, test_row, num_neighbors)\n",
    "    output_values_counter = Counter(y_train[neighbor_indicies])\n",
    "    prediction = output_values_counter.most_common()\n",
    "    return prediction[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data=[5.7, 2.9, 4.2, 1.3], Predicted: versicolor\n"
     ]
    }
   ],
   "source": [
    "# define model parameter\n",
    "num_neighbors = 5\n",
    "\n",
    "# define the test_row\n",
    "row = [5.7, 2.9, 4.2, 1.3]\n",
    "\n",
    "# predict the label\n",
    "label = predict_classification(iris_data.data, iris_data.target, row, num_neighbors)\n",
    "print('Data=%s, Predicted: %s' % (row, iris_data.target_names[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Norm and Data Scaling\n",
    "\n",
    "Euclidean norm is a great choice when:\n",
    "\n",
    "1. All the features are 'real' numbers.\n",
    "2. All the features are in the same scale.\n",
    "\n",
    "In our case, that is true.  All features are 'real' and in centimeters and seem to be in the same scale.  _However, you should scale your data before using it!_\n",
    "\n",
    "Two options are to use a MinMax or StandardScaler classes or user-defined procedures.\n",
    "\n",
    "If `x` is a numpy array of size $n \\times p$, then `min_by_column` is numpy array of size $p$ which are the minimums for each column.  Likewise for `max_by_column`.\n",
    "\n",
    "As before, `x2 - min_by_column` subtracts the $p$ array `min_by_column` from each row of `x2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale dataset columns to the range 0-1\n",
    "# dataset assumed to be a numpy array\n",
    "def normalize_array(x: np.array):\n",
    "    x2 = x.copy()\n",
    "    min_by_column = x2.min(axis=0)\n",
    "    max_by_column = x2.max(axis=0)\n",
    "    x2 = (x2 - min_by_column)/ (max_by_column - min_by_column)\n",
    "    return x2, min_by_column, max_by_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data=[5.7, 2.9, 4.2, 1.3], Predicted: versicolor\n"
     ]
    }
   ],
   "source": [
    "# predict the label using normalized data\n",
    "normalized_training_data, min_scaling_factor, max_scaling_factor = normalize_array(iris_data.data)\n",
    "normalized_row = (row - min_scaling_factor)/ (max_scaling_factor - min_scaling_factor)\n",
    "\n",
    "label = predict_classification(normalized_training_data, iris_data.target, normalized_row, num_neighbors)\n",
    "print('Data=%s, Predicted: %s' % (row, iris_data.target_names[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn: KNeighborsClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data=[5.7, 2.9, 4.2, 1.3], Predicted: ['versicolor']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris_data.data)\n",
    "\n",
    "normalized_training_data = scaler.transform(iris_data.data)\n",
    "#normalized test row\n",
    "normalized_row = scaler.transform(np.array(row).reshape(1,-1))\n",
    "\n",
    "# combination of 'minkowski' method and p=2 just means we're using Euclidean distance for our similarity measure\n",
    "#The Minkowski distance generalizes the Euclidean and the Manhatten distance in one distance metric. \n",
    "#If we set the parameter p in the following formula to 1 we get the manhattan distance an using the value 2 gives us the euclidean distance\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "knn.fit(normalized_training_data,iris_data.target)\n",
    "\n",
    "#now that we fit we can make out predictions\n",
    "label = knn.predict(normalized_row)\n",
    "\n",
    "### ENTER CODE HERE ###\n",
    "print('Data=%s, Predicted: %s' % (row, iris_data.target_names[label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Decision Region\n",
    "\n",
    "Let's display the decision regions predicted by the model.  We will only consider two dimensions, so let's work with petal length and petal width (the 3rd and 4th features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from Python Machine Learning, 2ed, Sebastian Raschka\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.66666667, 0.06779661, 0.04166667],\n",
       "       [0.30555556, 0.79166667, 0.11864407, 0.125     ],\n",
       "       [0.08333333, 0.58333333, 0.06779661, 0.08333333],\n",
       "       [0.19444444, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.02777778, 0.375     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.        ],\n",
       "       [0.30555556, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.13888889, 0.58333333, 0.10169492, 0.04166667],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.        ],\n",
       "       [0.        , 0.41666667, 0.01694915, 0.        ],\n",
       "       [0.41666667, 0.83333333, 0.03389831, 0.04166667],\n",
       "       [0.38888889, 1.        , 0.08474576, 0.125     ],\n",
       "       [0.30555556, 0.79166667, 0.05084746, 0.125     ],\n",
       "       [0.22222222, 0.625     , 0.06779661, 0.08333333],\n",
       "       [0.38888889, 0.75      , 0.11864407, 0.08333333],\n",
       "       [0.22222222, 0.75      , 0.08474576, 0.08333333],\n",
       "       [0.30555556, 0.58333333, 0.11864407, 0.04166667],\n",
       "       [0.22222222, 0.70833333, 0.08474576, 0.125     ],\n",
       "       [0.08333333, 0.66666667, 0.        , 0.04166667],\n",
       "       [0.22222222, 0.54166667, 0.11864407, 0.16666667],\n",
       "       [0.13888889, 0.58333333, 0.15254237, 0.04166667],\n",
       "       [0.19444444, 0.41666667, 0.10169492, 0.04166667],\n",
       "       [0.19444444, 0.58333333, 0.10169492, 0.125     ],\n",
       "       [0.25      , 0.625     , 0.08474576, 0.04166667],\n",
       "       [0.25      , 0.58333333, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.10169492, 0.04166667],\n",
       "       [0.13888889, 0.45833333, 0.10169492, 0.04166667],\n",
       "       [0.30555556, 0.58333333, 0.08474576, 0.125     ],\n",
       "       [0.25      , 0.875     , 0.08474576, 0.        ],\n",
       "       [0.33333333, 0.91666667, 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.5       , 0.03389831, 0.04166667],\n",
       "       [0.33333333, 0.625     , 0.05084746, 0.04166667],\n",
       "       [0.16666667, 0.66666667, 0.06779661, 0.        ],\n",
       "       [0.02777778, 0.41666667, 0.05084746, 0.04166667],\n",
       "       [0.22222222, 0.58333333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.625     , 0.05084746, 0.08333333],\n",
       "       [0.05555556, 0.125     , 0.05084746, 0.08333333],\n",
       "       [0.02777778, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.19444444, 0.625     , 0.10169492, 0.20833333],\n",
       "       [0.22222222, 0.75      , 0.15254237, 0.125     ],\n",
       "       [0.13888889, 0.41666667, 0.06779661, 0.08333333],\n",
       "       [0.22222222, 0.75      , 0.10169492, 0.04166667],\n",
       "       [0.08333333, 0.5       , 0.06779661, 0.04166667],\n",
       "       [0.27777778, 0.70833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.54166667, 0.06779661, 0.04166667],\n",
       "       [0.75      , 0.5       , 0.62711864, 0.54166667],\n",
       "       [0.58333333, 0.5       , 0.59322034, 0.58333333],\n",
       "       [0.72222222, 0.45833333, 0.66101695, 0.58333333],\n",
       "       [0.33333333, 0.125     , 0.50847458, 0.5       ],\n",
       "       [0.61111111, 0.33333333, 0.61016949, 0.58333333],\n",
       "       [0.38888889, 0.33333333, 0.59322034, 0.5       ],\n",
       "       [0.55555556, 0.54166667, 0.62711864, 0.625     ],\n",
       "       [0.16666667, 0.16666667, 0.38983051, 0.375     ],\n",
       "       [0.63888889, 0.375     , 0.61016949, 0.5       ],\n",
       "       [0.25      , 0.29166667, 0.49152542, 0.54166667],\n",
       "       [0.19444444, 0.        , 0.42372881, 0.375     ],\n",
       "       [0.44444444, 0.41666667, 0.54237288, 0.58333333],\n",
       "       [0.47222222, 0.08333333, 0.50847458, 0.375     ],\n",
       "       [0.5       , 0.375     , 0.62711864, 0.54166667],\n",
       "       [0.36111111, 0.375     , 0.44067797, 0.5       ],\n",
       "       [0.66666667, 0.45833333, 0.57627119, 0.54166667],\n",
       "       [0.36111111, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.41666667, 0.29166667, 0.52542373, 0.375     ],\n",
       "       [0.52777778, 0.08333333, 0.59322034, 0.58333333],\n",
       "       [0.36111111, 0.20833333, 0.49152542, 0.41666667],\n",
       "       [0.44444444, 0.5       , 0.6440678 , 0.70833333],\n",
       "       [0.5       , 0.33333333, 0.50847458, 0.5       ],\n",
       "       [0.55555556, 0.20833333, 0.66101695, 0.58333333],\n",
       "       [0.5       , 0.33333333, 0.62711864, 0.45833333],\n",
       "       [0.58333333, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.63888889, 0.41666667, 0.57627119, 0.54166667],\n",
       "       [0.69444444, 0.33333333, 0.6440678 , 0.54166667],\n",
       "       [0.66666667, 0.41666667, 0.6779661 , 0.66666667],\n",
       "       [0.47222222, 0.375     , 0.59322034, 0.58333333],\n",
       "       [0.38888889, 0.25      , 0.42372881, 0.375     ],\n",
       "       [0.33333333, 0.16666667, 0.47457627, 0.41666667],\n",
       "       [0.33333333, 0.16666667, 0.45762712, 0.375     ],\n",
       "       [0.41666667, 0.29166667, 0.49152542, 0.45833333],\n",
       "       [0.47222222, 0.29166667, 0.69491525, 0.625     ],\n",
       "       [0.30555556, 0.41666667, 0.59322034, 0.58333333],\n",
       "       [0.47222222, 0.58333333, 0.59322034, 0.625     ],\n",
       "       [0.66666667, 0.45833333, 0.62711864, 0.58333333],\n",
       "       [0.55555556, 0.125     , 0.57627119, 0.5       ],\n",
       "       [0.36111111, 0.41666667, 0.52542373, 0.5       ],\n",
       "       [0.33333333, 0.20833333, 0.50847458, 0.5       ],\n",
       "       [0.33333333, 0.25      , 0.57627119, 0.45833333],\n",
       "       [0.5       , 0.41666667, 0.61016949, 0.54166667],\n",
       "       [0.41666667, 0.25      , 0.50847458, 0.45833333],\n",
       "       [0.19444444, 0.125     , 0.38983051, 0.375     ],\n",
       "       [0.36111111, 0.29166667, 0.54237288, 0.5       ],\n",
       "       [0.38888889, 0.41666667, 0.54237288, 0.45833333],\n",
       "       [0.38888889, 0.375     , 0.54237288, 0.5       ],\n",
       "       [0.52777778, 0.375     , 0.55932203, 0.5       ],\n",
       "       [0.22222222, 0.20833333, 0.33898305, 0.41666667],\n",
       "       [0.38888889, 0.33333333, 0.52542373, 0.5       ],\n",
       "       [0.55555556, 0.54166667, 0.84745763, 1.        ],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.77777778, 0.41666667, 0.83050847, 0.83333333],\n",
       "       [0.55555556, 0.375     , 0.77966102, 0.70833333],\n",
       "       [0.61111111, 0.41666667, 0.81355932, 0.875     ],\n",
       "       [0.91666667, 0.41666667, 0.94915254, 0.83333333],\n",
       "       [0.16666667, 0.20833333, 0.59322034, 0.66666667],\n",
       "       [0.83333333, 0.375     , 0.89830508, 0.70833333],\n",
       "       [0.66666667, 0.20833333, 0.81355932, 0.70833333],\n",
       "       [0.80555556, 0.66666667, 0.86440678, 1.        ],\n",
       "       [0.61111111, 0.5       , 0.69491525, 0.79166667],\n",
       "       [0.58333333, 0.29166667, 0.72881356, 0.75      ],\n",
       "       [0.69444444, 0.41666667, 0.76271186, 0.83333333],\n",
       "       [0.38888889, 0.20833333, 0.6779661 , 0.79166667],\n",
       "       [0.41666667, 0.33333333, 0.69491525, 0.95833333],\n",
       "       [0.58333333, 0.5       , 0.72881356, 0.91666667],\n",
       "       [0.61111111, 0.41666667, 0.76271186, 0.70833333],\n",
       "       [0.94444444, 0.75      , 0.96610169, 0.875     ],\n",
       "       [0.94444444, 0.25      , 1.        , 0.91666667],\n",
       "       [0.47222222, 0.08333333, 0.6779661 , 0.58333333],\n",
       "       [0.72222222, 0.5       , 0.79661017, 0.91666667],\n",
       "       [0.36111111, 0.33333333, 0.66101695, 0.79166667],\n",
       "       [0.94444444, 0.33333333, 0.96610169, 0.79166667],\n",
       "       [0.55555556, 0.29166667, 0.66101695, 0.70833333],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 0.83333333],\n",
       "       [0.80555556, 0.5       , 0.84745763, 0.70833333],\n",
       "       [0.52777778, 0.33333333, 0.6440678 , 0.70833333],\n",
       "       [0.5       , 0.41666667, 0.66101695, 0.70833333],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.83333333],\n",
       "       [0.80555556, 0.41666667, 0.81355932, 0.625     ],\n",
       "       [0.86111111, 0.33333333, 0.86440678, 0.75      ],\n",
       "       [1.        , 0.75      , 0.91525424, 0.79166667],\n",
       "       [0.58333333, 0.33333333, 0.77966102, 0.875     ],\n",
       "       [0.55555556, 0.33333333, 0.69491525, 0.58333333],\n",
       "       [0.5       , 0.25      , 0.77966102, 0.54166667],\n",
       "       [0.94444444, 0.41666667, 0.86440678, 0.91666667],\n",
       "       [0.55555556, 0.58333333, 0.77966102, 0.95833333],\n",
       "       [0.58333333, 0.45833333, 0.76271186, 0.70833333],\n",
       "       [0.47222222, 0.41666667, 0.6440678 , 0.70833333],\n",
       "       [0.72222222, 0.45833333, 0.74576271, 0.83333333],\n",
       "       [0.66666667, 0.45833333, 0.77966102, 0.95833333],\n",
       "       [0.72222222, 0.45833333, 0.69491525, 0.91666667],\n",
       "       [0.41666667, 0.29166667, 0.69491525, 0.75      ],\n",
       "       [0.69444444, 0.5       , 0.83050847, 0.91666667],\n",
       "       [0.66666667, 0.54166667, 0.79661017, 1.        ],\n",
       "       [0.66666667, 0.41666667, 0.71186441, 0.91666667],\n",
       "       [0.55555556, 0.20833333, 0.6779661 , 0.75      ],\n",
       "       [0.61111111, 0.41666667, 0.71186441, 0.79166667],\n",
       "       [0.52777778, 0.58333333, 0.74576271, 0.91666667],\n",
       "       [0.44444444, 0.41666667, 0.69491525, 0.70833333]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "\n",
    "X = normalized_training_data[:,[2,3]] #all the rows of columns 3 and 4\n",
    "\n",
    "knn2.fit(X,iris_data.target)\n",
    "\n",
    "plot_decision_regions(X, iris_data.target, \n",
    "                      classifier=knn2)\n",
    "\n",
    "plt.xlabel('petal length [standardized]')\n",
    "plt.ylabel('petal width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: KNN for Spelling Correction\n",
    "\n",
    "Here, we ask the KNN algorithm to correct misspellings such as Holpful (should be Helpful).  To do so, we use a British-English [list of words](https://www.python-course.eu/british-english.txt) that was modified from [another source](https://www.python-course.eu/k_nearest_neighbor_classifier.php). \n",
    "\n",
    "Here, the input are not real numbers, but strings.  _So we cannot use Scikit-Learn, even with a custom distance function._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AB's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABM's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99151</th>\n",
       "      <td>épée's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99152</th>\n",
       "      <td>épées</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99153</th>\n",
       "      <td>étude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99154</th>\n",
       "      <td>étude's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99155</th>\n",
       "      <td>études</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99156 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word\n",
       "0            A\n",
       "1          A's\n",
       "2         AA's\n",
       "3         AB's\n",
       "4        ABM's\n",
       "...        ...\n",
       "99151   épée's\n",
       "99152    épées\n",
       "99153    étude\n",
       "99154  étude's\n",
       "99155   études\n",
       "\n",
       "[99156 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df = pd.read_csv(r'data/british-english.txt', names=[\"word\"])\n",
    "words_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the 'levenshtein' distance metric.  Check out [this video](https://www.youtube.com/watch?v=MiqoA-yF-0M) to learn more about the levenshtein distance, which measures the cost of changing one string to another given three basic operations: delete, replace, insert.  It shows how many changes are needed to get from string `s` to string `t`.  The following implementation comes from [here](https://www.python-course.eu/levenshtein_distance.php), where you'll find more clever implementations.\n",
    "\n",
    "Note that in the following code uses [type annotations](https://dev.to/dstarner/using-pythons-type-annotations-4cfe), which can be summarized by the following two additions to a function definition:\n",
    "\n",
    "- After the name of an input argument is defined in the function signature, it is followed by a colon and its data type - the general usage being: `<varName> : <varType>`.  For example, `myString : str`.  This is done to make the code clearer to read, and reveal it's purpose a little more.\n",
    "\n",
    "- Also, an arrow followed by a data type is added to the end of the function signature. The general usage being: `<funcName(p1, ..., pn)> -> <returnVarType>`.  For example, `stringFunc(myStr : str) -> int` indicates a function that accepts one input parameter called `myStr` of type `str` that returns a value of `int` data type. This method more easily shows the return value types of any function or method, to avoid confusion by future developers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenshtein calculates the the number of substitutions and deletions needed in order to transform one string into another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, List, Any, Callable\n",
    "\n",
    "memo = dict()   # type: Dict[Tuple[str, str], int]\n",
    "def levenshtein(s: str, t: str) -> int:\n",
    "    if s == \"\":\n",
    "        return len(t)\n",
    "    if t == \"\":\n",
    "        return len(s)\n",
    "    cost = 0 if s[-1] == t[-1] else 1\n",
    "\n",
    "    i1 = (s[:-1], t)\n",
    "    if i1 not in memo:\n",
    "        memo[i1] = levenshtein(*i1)\n",
    "    i2 = (s, t[:-1])\n",
    "    if i2 not in memo:\n",
    "        memo[i2] = levenshtein(*i2)\n",
    "    i3 = (s[:-1], t[:-1])\n",
    "    if i3 not in memo:\n",
    "        memo[i3] = levenshtein(*i3)\n",
    "    res = min([memo[i1] + 1, memo[i2] + 1, memo[i3] + cost])\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more general `get_neighbors()` that expects a distance function as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(training_set: List[Any],\n",
    "                  labels: List[Any],\n",
    "                  test_instance: Any,\n",
    "                  k: int,\n",
    "                  distance: Callable) -> List[Tuple[Any, float, Any]]:\n",
    "\n",
    "    distances = []\n",
    "    for index in range(len(training_set)):\n",
    "        dist = distance(test_instance, training_set[index])\n",
    "        distances.append((training_set[index], dist, labels[index]))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    neighbors = distances[:k]\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted by distance.  We will show this for the sci-kit learn code as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_distance_weights(neighbors: List[Tuple[Any, float, Any]], all_results: bool = True):\n",
    "    class_counter = Counter()\n",
    "    number_of_neighbors = len(neighbors)\n",
    "    for index in range(number_of_neighbors):\n",
    "        dist = neighbors[index][1]\n",
    "        label = neighbors[index][2]\n",
    "        class_counter[label] += 1 / (dist**2 + 1)\n",
    "    labels, votes = zip(*class_counter.most_common())\n",
    "    # print(labels, votes)\n",
    "    winner = class_counter.most_common(1)[0][0]\n",
    "    votes4winner = class_counter.most_common(1)[0][1]\n",
    "    if all_results:\n",
    "        total = sum(class_counter.values(), 0.0)\n",
    "        for key in class_counter:\n",
    "            class_counter[key] /= total\n",
    "        return winner, class_counter.most_common()\n",
    "    else:\n",
    "        return winner, votes4winner / sum(votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vote probability, unweighted by distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_prob(neighbors: List[Tuple[Any, float, Any]]):\n",
    "    class_counter = Counter()\n",
    "    for neighbor in neighbors:\n",
    "        class_counter[neighbor[2]] += 1\n",
    "    labels, votes = zip(*class_counter.most_common())\n",
    "    winner = class_counter.most_common()[0][0]\n",
    "    votes4winner = class_counter.most_common()[0][1]\n",
    "    return winner, votes4winner/sum(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# double check if there are any null values\n",
    "print(words_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# let's drop the bad values\n",
    "words_df.dropna(inplace=True)\n",
    "print(words_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datq      :\n",
      "  neighbors: [(1, 'data'), (1, 'date'), (2, 'Batu')]\n",
      "  vote_distance_weights: ('data', 0.4166666666666667)\n",
      "  vote_prob: ('data', 0.3333333333333333)\n",
      "sceince   :\n",
      "  neighbors: [(2, 'scene'), (2, 'science'), (2, 'sconce')]\n",
      "  vote_distance_weights: ('scene', 0.3333333333333333)\n",
      "  vote_prob: ('scene', 0.3333333333333333)\n",
      "holpful   :\n",
      "  neighbors: [(1, 'helpful'), (2, 'doleful'), (2, 'hopeful')]\n",
      "  vote_distance_weights: ('helpful', 0.5555555555555556)\n",
      "  vote_prob: ('helpful', 0.3333333333333333)\n",
      "kundnoss  :\n",
      "  neighbors: [(2, 'kindness'), (3, 'fondness'), (3, 'kudos')]\n",
      "  vote_distance_weights: ('kindness', 0.5)\n",
      "  vote_prob: ('kindness', 0.3333333333333333)\n",
      "holpposs  :\n",
      "  neighbors: [(3, 'helpless'), (3, \"hippo's\"), (3, 'hippos')]\n",
      "  vote_distance_weights: ('helpless', 0.3333333333333333)\n",
      "  vote_prob: ('helpless', 0.3333333333333333)\n",
      "blagrufoo :\n",
      "  neighbors: [(4, 'barefoot'), (5, 'Baguio'), (5, 'Blackfoot')]\n",
      "  vote_distance_weights: ('barefoot', 0.4333333333333333)\n",
      "  vote_prob: ('barefoot', 0.3333333333333333)\n",
      "qwerasdfzx:\n",
      "  neighbors: [(6, 'Amerasian'), (6, 'Fernandez'), (6, 'Gerald')]\n",
      "  vote_distance_weights: ('Amerasian', 0.3333333333333333)\n",
      "  vote_prob: ('Amerasian', 0.3333333333333333)\n"
     ]
    }
   ],
   "source": [
    "words = list(word.strip() for word in words_df[\"word\"])\n",
    "for word in [\"datq\", \"sceince\", \"holpful\", \"kundnoss\", \"holpposs\", \"blagrufoo\", \"qwerasdfzx\"]:\n",
    "    print(f\"{word:<10}:\")\n",
    "    k_neighbors = get_neighbors(words,\n",
    "                                words,\n",
    "                                word,\n",
    "                                3,\n",
    "                                distance=levenshtein)\n",
    "    print(f\"  neighbors: {[neighbor[1:] for neighbor in k_neighbors]}\")\n",
    "    print(f\"  vote_distance_weights: {vote_distance_weights(k_neighbors, all_results=False)}\")\n",
    "    print(f\"  vote_prob: {vote_prob(k_neighbors)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Choose K\n",
    "\n",
    "By convention, many people try using `K = 5`.  But you should certainly perform a more scientific analysis.  The below code employs various `K` values and performs cross-validation using accuracy as the test metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9533333333333334,\n",
       " 0.9533333333333334,\n",
       " 0.9533333333333334,\n",
       " 0.96,\n",
       " 0.9600000000000002,\n",
       " 0.9533333333333334,\n",
       " 0.96,\n",
       " 0.9666666666666666,\n",
       " 0.9666666666666666,\n",
       " 0.9533333333333334,\n",
       " 0.9533333333333334,\n",
       " 0.9533333333333334,\n",
       " 0.9466666666666667,\n",
       " 0.9466666666666667,\n",
       " 0.96]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#list starting at 1 abd ending at 29 and skipping by 2\n",
    "num_neighbors = list(range(1,30,2))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = [ ]\n",
    "\n",
    "#perform 10-fold cross-validation\n",
    "for k in num_neighbors:\n",
    "    knn4 = KNeighborsClassifier(n_neighbors=k, p=2, metric='minkowski')\n",
    "    \n",
    "\n",
    "    knn4.fit(normalized_training_data, iris_data.target)\n",
    "    \n",
    "    scores = cross_val_score(knn4, normalized_training_data, iris_data.target, cv=10, scoring='accuracy')\n",
    "    \n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy\n",
      "K           \n",
      "1   0.953333\n",
      "3   0.953333\n",
      "5   0.953333\n",
      "7   0.960000\n",
      "9   0.960000\n",
      "11  0.953333\n",
      "13  0.960000\n",
      "15  0.966667\n",
      "17  0.966667\n",
      "19  0.953333\n",
      "21  0.953333\n",
      "23  0.953333\n",
      "25  0.946667\n",
      "27  0.946667\n",
      "29  0.960000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAF3CAYAAABzKJvwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAahUlEQVR4nO3df6zddZ3n8ed77m1TAZECXcJwC+1uqm3tD62X4qKjk4huARWROJYwJTJgQ7Qd3CzZdJkNyzLZhE12R9EQG2bpIi62ZFCQUTI4iAwzykALlNraMlOh0GsRasnQIqml7Xv/OAdy5nJ774Hee97nts9HcsL5/jrf1zn3c29ffL/f+72RmUiSJHXa71UHkCRJRydLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSI5aQiFgVES9GxMZDLI+I+HpEbI2IDRGxoGXZooh4qrlsxWgGlyRJ41s7R0JuBRYNs/xcYEbzsRT4JkBE9AA3NZfPBi6OiNmHE1aSJB05RiwhmfkQ8NIwq1wA3JYN/wicEBGnAguBrZn5dGbuA9Y015UkSRqVa0JOA7a3TA805x1qviRJEr2j8BoxxLwcZv7QLxKxlMbpHI499tgPzJw5cxSiSZKkao899thvMnPK4PmjUUIGgKkt033ADmDiIeYPKTNvBm4G6O/vz3Xr1o1CNEmSVC0inh1q/micjrkHuLT5WzIfBF7OzOeBtcCMiJgeEROBxc11JUmSRj4SEhGrgT8ETo6IAeC/ARMAMnMlcC9wHrAVeBW4rLlsf0QsA+4DeoBVmblpDN6DJEkah0YsIZl58QjLE/jyIZbdS6OkSJIk/SujcU2IJEnj3muvvcbAwAB79+6tjjJuTZo0ib6+PiZMmNDW+pYQSZKAgYEB3vnOdzJt2jQihvoFTw0nM9m1axcDAwNMnz69rW382zGSJAF79+7lpJNOsoC8TRHBSSed9JaOJFlCJElqsoAcnrf6+VlCJEnqInfddRcRwZYtW6qjjDmvCZEkaQjTVvxwVF9v2w3nt7Xe6tWr+fCHP8yaNWu47rrrRjXD6w4cOEBPT8+YvPZb4ZEQSZK6xCuvvMJPf/pTbrnlFtasWQM0CsPVV1/N3LlzmTdvHt/4xjcAWLt2LWeffTbz589n4cKF7Nmzh1tvvZVly5a98Xqf/OQnefDBBwE47rjjuPbaaznrrLN4+OGHuf766znzzDOZM2cOS5cupXHHDdi6dSvnnHMO8+fPZ8GCBfzyl79kyZIlfP/733/jdS+55BLuuefw7z/qkRBJkrrE3XffzaJFi3j3u9/NiSeeyOOPP84jjzzCM888wxNPPEFvby8vvfQS+/bt4/Of/zx33HEHZ555Jrt37+Yd73jHsK/929/+ljlz5nD99dcDMHv2bK699loAlixZwg9+8AM+9alPcckll7BixQouvPBC9u7dy8GDB7niiiv46le/ygUXXMDLL7/Mz372M771rW8d9vv1SIgkSV1i9erVLF68GIDFixezevVq7r//fq688kp6exvHDU488USeeuopTj31VM4880wAjj/++DeWH0pPTw8XXXTRG9M/+clPOOuss5g7dy4PPPAAmzZtYs+ePfzqV7/iwgsvBBr3/TjmmGP46Ec/ytatW3nxxRdZvXo1F1100Yj7a4dHQiRJ6gK7du3igQceYOPGjUQEBw4cICL4wAc+8KbfOsnMIX8Tpbe3l4MHD74x3frrspMmTXrjOpC9e/fypS99iXXr1jF16lSuu+469u7d+8YpmaEsWbKE22+/nTVr1rBq1arDfbuAR0IkSeoKd955J5deeinPPvss27ZtY/v27UyfPp0FCxawcuVK9u/fD8BLL73EzJkz2bFjB2vXrgVgz5497N+/n2nTprF+/XoOHjzI9u3befTRR4fc1+vl5OSTT+aVV17hzjvvBBpHVPr6+rj77rsB+N3vfserr74KwBe+8AW+9rWvAfDe9753VN6zJUSSpC6wevXqN06DvO6iiy5ix44dnH766cybN4/58+fzne98h4kTJ3LHHXewfPly5s+fz8c//nH27t3Lhz70IaZPn87cuXO5+uqrWbBgwZD7OuGEE/jiF7/I3Llz+cxnPvPGaR2Ab3/723z9619n3rx5nH322fz6178G4JRTTmHWrFlcdtllo/aeY7hDL1X6+/tz3bp11TEkSUeRzZs3M2vWrOoYXevVV19l7ty5PP7447zrXe865HpDfY4R8Vhm9g9e1yMhkiRpWPfffz8zZ85k+fLlwxaQt8oLUyVJ0rDOOeccnnvuuVF/XY+ESJKkEpYQSZKauvE6yfHkrX5+lhBJkmjcR2PXrl0WkbcpM9m1axeTJk1qexuvCZEkCejr62NgYICdO3dWRxm3Jk2aRF9fX9vrW0IkSQImTJjA9OnTq2McVTwdI0mSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEkl2iohEbEoIp6KiK0RsWKI5ZMj4q6I2BARj0bEnJZl/zEiNkXExohYHRGTRvMNSJKk8WnEEhIRPcBNwLnAbODiiJg9aLVrgPWZOQ+4FLixue1pwJ8C/Zk5B+gBFo9efEmSNF61cyRkIbA1M5/OzH3AGuCCQevMBn4MkJlbgGkRcUpzWS/wjojoBY4BdoxKckmSNK61U0JOA7a3TA8057V6EvgsQEQsBM4A+jLzV8D/Ap4DngdezswfHW5oSZI0/rVTQmKIeTlo+gZgckSsB5YDTwD7I2IyjaMm04HfB46NiD8ecicRSyNiXUSs27lzZ9tvQJIkjU/tlJABYGrLdB+DTqlk5u7MvCwz30fjmpApwDPAOcAzmbkzM18DvgecPdROMvPmzOzPzP4pU6a8jbciSZLGk3ZKyFpgRkRMj4iJNC4svad1hYg4obkM4ArgoczcTeM0zAcj4piICOBjwObRiy9Jksar3pFWyMz9EbEMuI/Gb7esysxNEXFlc/lKYBZwW0QcAH4BXN5c9khE3Ak8DuyncZrm5jF5J5IkaVyJzMGXd9Tr7+/PdevWVceQJEmjICIey8z+wfO9Y6okSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUorc6gKQj07QVPzzs19h2w/lHTA5Jb+aREEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSrRVQiJiUUQ8FRFbI2LFEMsnR8RdEbEhIh6NiDkty06IiDsjYktEbI6Ifz+ab0CSJI1PI5aQiOgBbgLOBWYDF0fE7EGrXQOsz8x5wKXAjS3LbgT+JjNnAvOBzaMRXJIkjW/tHAlZCGzNzKczcx+wBrhg0DqzgR8DZOYWYFpEnBIRxwMfAW5pLtuXmf8yauklSdK41dvGOqcB21umB4CzBq3zJPBZ4B8iYiFwBtAHHAB2Av83IuYDjwFXZeZvB+8kIpYCSwFOP/30t/g2VGnaih8e9mtsu+H8IyZHN/CzkNSuw/15cTg/K9o5EhJDzMtB0zcAkyNiPbAceALYT6PkLAC+mZnvB34LvOmaEoDMvDkz+zOzf8qUKe3mlyRJ41Q7R0IGgKkt033AjtYVMnM3cBlARATwTPNxDDCQmY80V72TQ5QQSZJ0dGnnSMhaYEZETI+IicBi4J7WFZq/ATOxOXkF8FBm7s7MXwPbI+I9zWUfA34xStklSdI4NuKRkMzcHxHLgPuAHmBVZm6KiCuby1cCs4DbIuIAjZJxectLLAdub5aUp2keMZEkSUe3dk7HkJn3AvcOmrey5fnDwIxDbLse6D+MjJIk6QjkHVMlSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJK9FYHeDumrfjhYb/GthvOH/cZuimHGvx6aCjdMi4ON0c3ZBitHOoOHgmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVKK3OoAkSZ02bcUPD2v7bTecX55htHJU8kiIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkq0VYJiYhFEfFURGyNiBVDLJ8cEXdFxIaIeDQi5gxa3hMRT0TED0YruCRJGt9GLCER0QPcBJwLzAYujojZg1a7BlifmfOAS4EbBy2/Cth8+HElSdKRop0jIQuBrZn5dGbuA9YAFwxaZzbwY4DM3AJMi4hTACKiDzgf+D+jllqSJI177ZSQ04DtLdMDzXmtngQ+CxARC4EzgL7msq8B/xk4ONxOImJpRKyLiHU7d+5sI5YkSRrP2ikhMcS8HDR9AzA5ItYDy4EngP0R8Ungxcx8bKSdZObNmdmfmf1TpkxpI5YkSRrPettYZwCY2jLdB+xoXSEzdwOXAUREAM80H4uBT0fEecAk4PiI+H+Z+cejkF2SJI1j7RwJWQvMiIjpETGRRrG4p3WFiDihuQzgCuChzNydmf8lM/syc1pzuwcsIJIkCdo4EpKZ+yNiGXAf0AOsysxNEXFlc/lKYBZwW0QcAH4BXD6GmSVJ0hGgndMxZOa9wL2D5q1sef4wMGOE13gQePAtJ5QkSUck75gqSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUoq0SEhGLIuKpiNgaESuGWD45Iu6KiA0R8WhEzGnOnxoRP4mIzRGxKSKuGu03IEmSxqcRS0hE9AA3AecCs4GLI2L2oNWuAdZn5jzgUuDG5vz9wH/KzFnAB4EvD7GtJEk6CrVzJGQhsDUzn87MfcAa4IJB68wGfgyQmVuAaRFxSmY+n5mPN+fvATYDp41aekmSNG61U0JOA7a3TA/w5iLxJPBZgIhYCJwB9LWuEBHTgPcDj7y9qJIk6UjSTgmJIebloOkbgMkRsR5YDjxB41RM4wUijgO+C3wlM3cPuZOIpRGxLiLW7dy5s63wkiRp/OptY50BYGrLdB+wo3WFZrG4DCAiAnim+SAiJtAoILdn5vcOtZPMvBm4GaC/v39wyZEkSUeYdo6ErAVmRMT0iJgILAbuaV0hIk5oLgO4AngoM3c3C8ktwObM/IvRDC5Jksa3EY+EZOb+iFgG3Af0AKsyc1NEXNlcvhKYBdwWEQeAXwCXNzf/ELAE+HnzVA3ANZl57yi/D0mSNM60czqGZmm4d9C8lS3PHwZmDLHdPzD0NSWSJOko5x1TJUlSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJWwhEiSpBKWEEmSVMISIkmSSrRVQiJiUUQ8FRFbI2LFEMsnR8RdEbEhIh6NiDntbitJko5OI5aQiOgBbgLOBWYDF0fE7EGrXQOsz8x5wKXAjW9hW0mSdBRq50jIQmBrZj6dmfuANcAFg9aZDfwYIDO3ANMi4pQ2t5UkSUehdkrIacD2lumB5rxWTwKfBYiIhcAZQF+b20qSpKNQZObwK0R8DvgPmXlFc3oJsDAzl7esczyNUzDvB34OzASuAN490rYtr7EUWNqcfA/w1GG8r5OB3xzG9qOlG3J0QwbojhzdkAG6I0c3ZIDuyNENGaA7cnRDBuiOHN2QAbojx2hkOCMzpwye2dvGhgPA1JbpPmBH6wqZuRu4DCAiAnim+ThmpG1bXuNm4OY28owoItZlZv9ovNZ4z9ENGbolRzdk6JYc3ZChW3J0Q4ZuydENGbolRzdk6JYcY5mhndMxa4EZETE9IiYCi4F7BgU8obkMGkdAHmoWkxG3lSRJR6cRj4Rk5v6IWAbcB/QAqzJzU0Rc2Vy+EpgF3BYRB4BfAJcPt+3YvBVJkjSetHM6hsy8F7h30LyVLc8fBma0u20HjMppnVHQDTm6IQN0R45uyADdkaMbMkB35OiGDNAdObohA3RHjm7IAN2RY8wyjHhhqiRJ0ljwtu2SJKnEEVVCImJVRLwYERsLM0xq3rr+yYjYFBH/vTDLtoj4eUSsj4h1Bft/T3Pfrz92R8RXOp2jmeWqiNjY/Jp0JMNQ4zEiPtfMcDAiOnLF+yFy/Hnzzyysj4gfRcTvF2S4LiJ+1TI+zhvLDMPkuKMlw7aIWF+QYX5EPNz8fv3r5m0PxjLD1Ij4SURsbo7Hq5rzOzo+h8nRsfE5TIaOjs9hcnRsfA6TYezGZ2YeMQ/gI8ACYGNhhgCOaz6fADwCfLAoyzbg5OqvSzNLD/BrGr8r3ul9zwE20viV8V7gfmBGB/b7pvFI4yLu9wAPAv0dev9D5Ti+5fmfAisLMlwHXN3hsTDszwjgfwPXFnwWa4GPNp//CfDnY5zhVGBB8/k7gX+icefrjo7PYXJ0bHwOk6Gj4/NQOQatM6bjc5jPYszG5xF1JCQzHwJeKs6QmflKc3JC8+GFN/Ax4JeZ+WzBvmcB/5iZr2bmfuDvgAvHeqdDjcfM3JyZh3MjvtHKsbtl8ljGeIx2w/fmSDkiIoA/AlYXZHgP8FDz+d8CF41xhucz8/Hm8z3AZuC0To/PYXJ0bHweKsNY7e/t5ujE+Bwmw5iNzyOqhHSLiOhpHjJ7EfjbzHykKEoCP4qIx6JxR9pKixnjH+7D2Ah8JCJOiohjgPP41zfROypFxP+IiO3AJcC1RTGWNQ+7r4qIyUUZXvcHwAuZ+c8F+94IfLr5/HN0cHxGxDQad7uu+jk1ZI6K8TnEZ1EyPg/xNeno+ByUYczGpyVkDGTmgcx8H407xC6MiDlFUT6UmQto/BXjL0fERypCRONGdZ8G/qpi/5m5GfifNBr839D4W0f7K7J0k8z8s8ycCtwOLCuI8E3g3wHvA56ncai50sXUFeU/ofE9+hiNw+D7OrHTiDgO+C7wlUFHHzpqqBydHp9DZCgZn8N8TTo2PofIMGbj0xIyhjLzX2icW11UtP8dzf++CNxF468aVzgXeDwzXyjaP5l5S2YuyMyP0DgUXvF/u93qO4zx4f+hZOYLzcJ+EPhL6sYnEdFL449w3lGx/8zckpmfyMwP0PiH5pdjvc+ImEDjH5rbM/N7Y72/w8gx5uNzqAwV4/NQn0Unx+chPosxG5+WkFEWEVMi4oTm83cA5wBbCnIcGxHvfP058Akah9QqVP4fJgAR8W+a/z2dxjdzaZ5qEdF6c8FPUzNGT22ZvJC68QnN79PMHKjYecv4/D3gvwIrh9/isPcXwC3A5sz8i7Hc19vJ0cnxOUyGjo7PEb4mHRmfw3wWYzc+x+oq24oHjX9Yngdeo/GH9y4vyDAPeALYQGPQjumV9sPk+Lc0Tjs8CWwC/qwoxzHALuBdxWPj72n8SYEngY91aJ9vGo80fpgNAL8DXgDuK8rx3eb43AD8NY2LATud4ds0/ur2Bhp/U+rUis+iOf9W4MrCcXEVjd9E+CfgBpo3khzDDB+mcc3YBmB983Fep8fnMDk6Nj6HydDR8XmoHJ0cn8N8FmM2Pr1jqiRJKuHpGEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSOioiXml5fl5E/HPz/i2SjjK91QEkHZ0i4mPAN4BPZOZz1XkkdZ4lRFLHRcQf0LgV9nmZOea3KJfUnbxZmaSOiojXgD3AH2bmhuo8kup4TYikTnsN+BmN25VLOopZQiR12kHgj4AzI+Ka6jCS6nhNiKSOy8xXI+KTwN9HxAuZeUt1JkmdZwmRVCIzX4qIRcBDEfGbzPx+dSZJneWFqZIkqYTXhEiSpBKWEEmSVMISIkmSSlhCJElSCUuIJEkqYQmRJEklLCGSJKmEJUSSJJX4/1FX6d1TdwoIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_scores_df = pd.DataFrame({\"K\": [k for k in num_neighbors], \n",
    "              \"Accuracy\": cv_scores}).set_index(\"K\")\n",
    "print(cv_scores_df)\n",
    "cv_scores_df.plot.bar(figsize=(9,6), ylim=(0.9, 1.0), rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal no. of neighbors is 15\n"
     ]
    }
   ],
   "source": [
    "# Changing to misclassification error\n",
    "mse = [1-x for x in cv_scores]\n",
    "\n",
    "# determing best k\n",
    "optimal_k = num_neighbors[mse.index(min(mse))]\n",
    "print(\"The optimal no. of neighbors is {}\".format(optimal_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance-Weighted KNN\n",
    "\n",
    "Perhaps a way to make the solution less sensitive to the 'right' choice of $k$ is to weight the value by distance.\n",
    "\n",
    "If we pick $k=5$, and the nearest neighbors give labels of \\[A, A, B, B, B\\], we would be inclided to pick the majority class of B.\n",
    "\n",
    "But what if the two A's were really close, and the three B's were somewhat further away?  We might pick A over B.\n",
    "\n",
    "We can use the `weights='distance'` hyperparameter, which weights data points by the inverse of their distance.  In this case, closer neighbors of a query point will have a greater influence than neighbors which are further away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy\n",
      "K           \n",
      "1   0.953333\n",
      "3   0.953333\n",
      "5   0.953333\n",
      "7   0.953333\n",
      "9   0.946667\n",
      "11  0.953333\n",
      "13  0.953333\n",
      "15  0.960000\n",
      "17  0.960000\n",
      "19  0.953333\n",
      "21  0.953333\n",
      "23  0.953333\n",
      "25  0.953333\n",
      "27  0.953333\n",
      "29  0.960000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAF3CAYAAABzKJvwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAafElEQVR4nO3df6zV9Z3n8ed77oVQtVZU1jheFHZDC5QfLV6xazvtJLVd/NFaNZ1iHEwdLTEtjN2s2bDOxnWdbOImu9PaxpQ4K2vtWjBjq3VaM3bUOs60joKKFArO4I+WK1opZgrWUATe+8c5mjO3l3uPcu95nwvPR3Lj+f445/s6h8+Fl5/v935vZCaSJEmd9nvVASRJ0pHJEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSpxIglJCJWRcQrEbHxINsjIr4WEVsjYkNELGjZtiginmluWzGawSVJ0vjWzkzIbcCiYbafA8xofi0FvgEQET3Azc3ts4FLImL2oYSVJEmHjxFLSGY+Arw6zC4XALdnwz8Cx0XEycBCYGtmPpeZe4E1zX0lSZJG5ZqQU4BtLcsDzXUHWy9JkkTvKLxGDLEuh1k/9ItELKVxOoejjz769JkzZ45CNEmSVO2JJ574VWZOGbx+NErIADC1ZbkP2A5MPMj6IWXmLcAtAP39/blu3bpRiCZJkqpFxM+HWj8ap2PuBS5r/pTMh4BfZ+ZLwFpgRkRMj4iJwOLmvpIkSSPPhETEauAPgRMjYgD4b8AEgMxcCdwHnAtsBV4HLm9u2xcRy4D7gR5gVWZuGoP3IEmSxqERS0hmXjLC9gS+dJBt99EoKZIkSf/KaFwTIknSuPfGG28wMDDAnj17qqOMW5MmTaKvr48JEya0tb8lRJIkYGBggHe/+91MmzaNiKF+wFPDyUx27tzJwMAA06dPb+s5/u4YSZKAPXv2cMIJJ1hA3qGI4IQTTnhbM0mWEEmSmiwgh+btfn6WEEmSusjdd99NRLBly5bqKGPOa0IkSRrCtBU/GNXXe+HG89rab/Xq1XzkIx9hzZo1XH/99aOa4U379++np6dnTF777XAmRJKkLvHaa6/x4x//mFtvvZU1a9YAjcJwzTXXMHfuXObNm8fXv/51ANauXctZZ53F/PnzWbhwIbt37+a2225j2bJlb73e+eefz8MPPwzAMcccw3XXXceZZ57Jo48+yg033MAZZ5zBnDlzWLp0KY07bsDWrVs5++yzmT9/PgsWLODZZ59lyZIlfO9733vrdS+99FLuvffQ7z/qTIgkSV3innvuYdGiRbz3ve/l+OOP58knn+Sxxx7j+eef56mnnqK3t5dXX32VvXv38rnPfY4777yTM844g127dvGud71r2Nf+zW9+w5w5c7jhhhsAmD17Ntdddx0AS5Ys4fvf/z6f+tSnuPTSS1mxYgUXXnghe/bs4cCBA1x55ZV85Stf4YILLuDXv/41P/nJT/jmN795yO/XmRBJkrrE6tWrWbx4MQCLFy9m9erVPPDAA1x11VX09jbmDY4//nieeeYZTj75ZM444wwAjj322Le2H0xPTw8XX3zxW8s/+tGPOPPMM5k7dy4PPfQQmzZtYvfu3bz44otceOGFQOO+H0cddRQf+9jH2Lp1K6+88gqrV6/m4osvHvF47XAmRJKkLrBz504eeughNm7cSESwf/9+IoLTTz/9d37qJDOH/EmU3t5eDhw48NZy64/LTpo06a3rQPbs2cMXv/hF1q1bx9SpU7n++uvZs2fPW6dkhrJkyRLuuOMO1qxZw6pVqw717QLOhEiS1BXuuusuLrvsMn7+85/zwgsvsG3bNqZPn86CBQtYuXIl+/btA+DVV19l5syZbN++nbVr1wKwe/du9u3bx7Rp01i/fj0HDhxg27ZtPP7440Me681ycuKJJ/Laa69x1113AY0Zlb6+Pu655x4Afvvb3/L6668D8PnPf56vfvWrALz//e8flfdsCZEkqQusXr36rdMgb7r44ovZvn07p556KvPmzWP+/Pl8+9vfZuLEidx5550sX76c+fPn84lPfII9e/bw4Q9/mOnTpzN37lyuueYaFixYMOSxjjvuOL7whS8wd+5cPvOZz7x1WgfgW9/6Fl/72teYN28eZ511Fi+//DIAJ510ErNmzeLyyy8ftfccw029VOnv789169ZVx5AkHUE2b97MrFmzqmN0rddff525c+fy5JNP8p73vOeg+w31OUbEE5nZP3hfZ0IkSdKwHnjgAWbOnMny5cuHLSBvlxemSpKkYZ199tn84he/GPXXdSZEkiSVsIRIktTUjddJjidv9/OzhEiSROM+Gjt37rSIvEOZyc6dO5k0aVLbz/GaEEmSgL6+PgYGBtixY0d1lHFr0qRJ9PX1tb2/JUSSJGDChAlMnz69OsYRxdMxkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklSirRISEYsi4pmI2BoRK4bYPjki7o6IDRHxeETMadn2HyNiU0RsjIjVETFpNN+AJEkan0YsIRHRA9wMnAPMBi6JiNmDdrsWWJ+Z84DLgJuazz0F+FOgPzPnAD3A4tGLL0mSxqt2ZkIWAlsz87nM3AusAS4YtM9s4EGAzNwCTIuIk5rbeoF3RUQvcBSwfVSSS5Kkca2dEnIKsK1leaC5rtXTwEUAEbEQOA3oy8wXgf8F/AJ4Cfh1Zv7wUENLkqTxr50SEkOsy0HLNwKTI2I9sBx4CtgXEZNpzJpMB34fODoi/njIg0QsjYh1EbFux44dbb8BSZI0PrVTQgaAqS3LfQw6pZKZuzLz8sz8AI1rQqYAzwNnA89n5o7MfAP4LnDWUAfJzFsysz8z+6dMmfIO3ookSRpP2ikha4EZETE9IibSuLD03tYdIuK45jaAK4FHMnMXjdMwH4qIoyIigI8Dm0cvviRJGq96R9ohM/dFxDLgfho/3bIqMzdFxFXN7SuBWcDtEbEf+BlwRXPbYxFxF/AksI/GaZpbxuSdSJKkcSUyB1/eUa+/vz/XrVtXHUOSJI2CiHgiM/sHr/eOqZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqURbJSQiFkXEMxGxNSJWDLF9ckTcHREbIuLxiJjTsu24iLgrIrZExOaI+Pej+QYkSdL4NGIJiYge4GbgHGA2cElEzB6027XA+sycB1wG3NSy7SbgbzJzJjAf2DwawSVJ0vjWzkzIQmBrZj6XmXuBNcAFg/aZDTwIkJlbgGkRcVJEHAt8FLi1uW1vZv7LqKWXJEnjVm8b+5wCbGtZHgDOHLTP08BFwD9ExELgNKAP2A/sAP5vRMwHngCuzszfDD5IRCwFlgKceuqpb/NtSOo201b84JBf44Ubzztsckjd6lC/Rw7l+6OdmZAYYl0OWr4RmBwR64HlwFPAPholZwHwjcz8IPAb4HeuKQHIzFsysz8z+6dMmdJufkmSNE61MxMyAExtWe4DtrfukJm7gMsBIiKA55tfRwEDmflYc9e7OEgJkSRJR5Z2ZkLWAjMiYnpETAQWA/e27tD8CZiJzcUrgUcyc1dmvgxsi4j3Nbd9HPjZKGWXJEnj2IgzIZm5LyKWAfcDPcCqzNwUEVc1t68EZgG3R8R+GiXjipaXWA7c0Swpz9GcMZEkSUe2dk7HkJn3AfcNWrey5fGjwIyDPHc90H8IGSVJ0mHIO6ZKkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSV6K0O8E5MW/GDQ36NF248b9xn6JYc3ZChW3TLZ9EtOdTQLX8eh5qjGzJ0S45uyDBaOSo5EyJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSpRG91AGm0TFvxg0N+jRduPG8UkkiS2uFMiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKtFWCYmIRRHxTERsjYgVQ2yfHBF3R8SGiHg8IuYM2t4TEU9FxPdHK7gkSRrfRiwhEdED3AycA8wGLomI2YN2uxZYn5nzgMuAmwZtvxrYfOhxJUnS4aKdmZCFwNbMfC4z9wJrgAsG7TMbeBAgM7cA0yLiJICI6APOA/7PqKWWJEnjXjsl5BRgW8vyQHNdq6eBiwAiYiFwGtDX3PZV4D8DB4Y7SEQsjYh1EbFux44dbcSSJEnjWTslJIZYl4OWbwQmR8R6YDnwFLAvIs4HXsnMJ0Y6SGbekpn9mdk/ZcqUNmJJkqTxrLeNfQaAqS3LfcD21h0ycxdwOUBEBPB882sx8OmIOBeYBBwbEf8vM/94FLJLkqRxrJ2ZkLXAjIiYHhETaRSLe1t3iIjjmtsArgQeycxdmflfMrMvM6c1n/eQBUSSJEEbMyGZuS8ilgH3Az3AqszcFBFXNbevBGYBt0fEfuBnwBVjmFmSJB0G2jkdQ2beB9w3aN3KlsePAjNGeI2HgYffdkJJknRY8o6pkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkpYQiRJUglLiCRJKmEJkSRJJdoqIRGxKCKeiYitEbFiiO2TI+LuiNgQEY9HxJzm+qkR8aOI2BwRmyLi6tF+A5IkaXwasYRERA9wM3AOMBu4JCJmD9rtWmB9Zs4DLgNuaq7fB/ynzJwFfAj40hDPlSRJR6B2ZkIWAlsz87nM3AusAS4YtM9s4EGAzNwCTIuIkzLzpcx8srl+N7AZOGXU0kuSpHGrnRJyCrCtZXmA3y0STwMXAUTEQuA0oK91h4iYBnwQeOydRZUkSYeTdkpIDLEuBy3fCEyOiPXAcuApGqdiGi8QcQzwHeDLmblryINELI2IdRGxbseOHW2FlyRJ41dvG/sMAFNblvuA7a07NIvF5QAREcDzzS8iYgKNAnJHZn73YAfJzFuAWwD6+/sHlxxJknSYaWcmZC0wIyKmR8REYDFwb+sOEXFccxvAlcAjmbmrWUhuBTZn5l+MZnBJkjS+jTgTkpn7ImIZcD/QA6zKzE0RcVVz+0pgFnB7ROwHfgZc0Xz6h4ElwE+bp2oArs3M+0b5fUiSpHGmndMxNEvDfYPWrWx5/CgwY4jn/QNDX1MiSZKOcN4xVZIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqYQlRJIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4gkSSphCZEkSSUsIZIkqURbJSQiFkXEMxGxNSJWDLF9ckTcHREbIuLxiJjT7nMlSdKRacQSEhE9wM3AOcBs4JKImD1ot2uB9Zk5D7gMuOltPFeSJB2B2pkJWQhszcznMnMvsAa4YNA+s4EHATJzCzAtIk5q87mSJOkI1E4JOQXY1rI80FzX6mngIoCIWAicBvS1+VxJknQEiswcfoeIzwL/ITOvbC4vARZm5vKWfY6lcQrmg8BPgZnAlcB7R3puy2ssBZY2F98HPHMI7+tE4FeH8PzR0g05uiEDdEeObsgA3ZGjGzJAd+TohgzQHTm6IQN0R45uyADdkWM0MpyWmVMGr+xt44kDwNSW5T5ge+sOmbkLuBwgIgJ4vvl11EjPbXmNW4Bb2sgzoohYl5n9o/Fa4z1HN2TolhzdkKFbcnRDhm7J0Q0ZuiVHN2TolhzdkKFbcoxlhnZOx6wFZkTE9IiYCCwG7h0U8LjmNmjMgDzSLCYjPleSJB2ZRpwJycx9EbEMuB/oAVZl5qaIuKq5fSUwC7g9IvYDPwOuGO65Y/NWJEnSeNLO6Rgy8z7gvkHrVrY8fhSY0e5zO2BUTuuMgm7I0Q0ZoDtydEMG6I4c3ZABuiNHN2SA7sjRDRmgO3J0QwbojhxjlmHEC1MlSZLGgrdtlyRJJQ6rEhIRqyLilYjYWJhhUvPW9U9HxKaI+O+FWV6IiJ9GxPqIWFdw/Pc1j/3m166I+HKnczSzXB0RG5t/Jh3JMNR4jIjPNjMciIiOXPF+kBx/3vw1C+sj4ocR8fsFGa6PiBdbxse5Y5lhmBx3tmR4ISLWF2SYHxGPNr9f/7p524OxzDA1In4UEZub4/Hq5vqOjs9hcnRsfA6ToaPjc5gcHRufw2QYu/GZmYfNF/BRYAGwsTBDAMc0H08AHgM+VJTlBeDE6j+XZpYe4GUaPyve6WPPATbS+JHxXuABYEYHjvs745HGRdzvAx4G+jv0/ofKcWzL4z8FVhZkuB64psNjYdi/I4D/DVxX8FmsBT7WfPwnwJ+PcYaTgQXNx+8G/onGna87Oj6HydGx8TlMho6Oz4PlGLTPmI7PYT6LMRufh9VMSGY+ArxanCEz87Xm4oTmlxfewMeBZzPz5wXHngX8Y2a+npn7gL8DLhzrgw41HjNzc2Yeyo34RivHrpbFoxnjMdoN35sj5YiIAP4IWF2Q4X3AI83HfwtcPMYZXsrMJ5uPdwObgVM6PT6HydGx8XmwDGN1vHeaoxPjc5gMYzY+D6sS0i0ioqc5ZfYK8LeZ+VhRlAR+GBFPROOOtJUWM8Z/uQ9jI/DRiDghIo4CzuVf30TviBQR/yMitgGXAtcVxVjWnHZfFRGTizK86Q+AX2bmPxcceyPw6ebjz9LB8RkR02jc7brq76khc1SMzyE+i5LxeZA/k46Oz0EZxmx8WkLGQGbuz8wP0LhD7MKImFMU5cOZuYDGbzH+UkR8tCJENG5U92ngryqOn5mbgf9Jo8H/DY3fdbSvIks3ycw/y8ypwB3AsoII3wD+HfAB4CUaU82VLqGuKP8Jje/RJ2hMg+/txEEj4hjgO8CXB80+dNRQOTo9PofIUDI+h/kz6dj4HCLDmI1PS8gYysx/oXFudVHR8bc3//sKcDeN32pc4Rzgycz8ZdHxycxbM3NBZn6UxlR4xf/tdqtvM8bT/0PJzF82C/sB4C+pG59ERC+NX8J5Z8XxM3NLZn4yM0+n8Q/Ns2N9zIiYQOMfmjsy87tjfbxDyDHm43OoDBXj82CfRSfH50E+izEbn5aQURYRUyLiuObjdwFnA1sKchwdEe9+8zHwSRpTahUq/w8TgIj4N83/nkrjm7k0T7WIaL254KepGaMntyxeSN34hOb3aWYOVBy8ZXz+HvBfgZXDP+OQjxfArcDmzPyLsTzWO8nRyfE5TIaOjs8R/kw6Mj6H+SzGbnyO1VW2FV80/mF5CXiDxi/eu6IgwzzgKWADjUE7plfaD5Pj39I47fA0sAn4s6IcRwE7gfcUj42/p/ErBZ4GPt6hY/7OeKTxl9kA8Fvgl8D9RTm+0xyfG4C/pnExYKczfIvGb93eQON3Sp1c8Vk0198GXFU4Lq6m8ZMI/wTcSPNGkmOY4SM0rhnbAKxvfp3b6fE5TI6Ojc9hMnR0fB4sRyfH5zCfxZiNT++YKkmSSng6RpIklbCESJKkEpYQSZJUwhIiSZJKWEIkSVIJS4ikjoqI11oenxsR/9y8f4ukI0xvdQBJR6aI+DjwdeCTmfmL6jySOs8SIqnjIuIPaNwK+9zMHPNblEvqTt6sTFJHRcQbwG7gDzNzQ3UeSXW8JkRSp70B/ITG7colHcEsIZI67QDwR8AZEXFtdRhJdbwmRFLHZebrEXE+8PcR8cvMvLU6k6TOs4RIKpGZr0bEIuCRiPhVZn6vOpOkzvLCVEmSVMJrQiRJUglLiCRJKmEJkSRJJSwhkiSphCVEkiSVsIRIkqQSlhBJklTCEiJJkkr8f5qC6dt90SpZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# empty list that will hold cv scores\n",
    "cv_scores = [ ]\n",
    "\n",
    "#perform 10-fold cross-validation\n",
    "for k in num_neighbors:\n",
    "    \n",
    "    knn5 = KNeighborsClassifier(n_neighbors=k, p=2, metric='minkowski', weights='distance')\n",
    "    \n",
    "\n",
    "    knn5.fit(normalized_training_data, iris_data.target)\n",
    "    \n",
    "    #cross fold validation for our new model \n",
    "    scores = cross_val_score(knn5, normalized_training_data, iris_data.target, cv=10, scoring='accuracy')\n",
    "    \n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "cv_scores_df = pd.DataFrame({\"K\": [k for k in num_neighbors], \n",
    "                             \"Accuracy\": cv_scores}).set_index(\"K\")\n",
    "print(cv_scores_df)\n",
    "cv_scores_df.plot.bar(figsize=(9,6), ylim=(0.9, 1.0), rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'optimal' $k$ is essentially the same.  But the overall graph is much smoother, justifying the idea that weighting might lead to less sensitivity in choosing $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
